{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')  #cd to project folder\n",
    "\n",
    "import pandas as pd\n",
    "from Bio import Phylo\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from requests.exceptions import HTTPError\n",
    "import time, logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "\n",
    "#import Angiosperms tree\n",
    "tree_file_path = Path('data/global_tree_brlen_pruned_renamed.tre')\n",
    "if tree_file_path.exists():\n",
    "    tree = Phylo.read(tree_file_path, 'newick')\n",
    "    logging.info(f'Angiosperms phylogenetic tree successfully imported!')\n",
    "else:\n",
    "    logging.warning(f'Tree file not found in {tree_file_path}.')\n",
    "\n",
    "#extract leaf names\n",
    "tree_leaves = [leaf.name for leaf in tree.get_terminals()]\n",
    "tree_leaves = pd.Series(tree_leaves, name='leaf_name')\n",
    "\n",
    "#create df with order, family, genus, species\n",
    "tree_df = tree_leaves.str.split('_', expand=True)\n",
    "tree_df = tree_df.iloc[:, :4] #keep first 4 columns\n",
    "tree_df.columns = ['Order', 'Family', 'Genus', 'Species'] #rename columns\n",
    "tree_df = pd.concat([tree_leaves, tree_df], axis=1).rename(columns={0: 'leaf_name'}).set_index('leaf_name')\n",
    "\n",
    "#list of genera in the tree to be queried in Wikidata\n",
    "genera_list = tree_df['Genus'].unique()\n",
    "\n",
    "#summary\n",
    "logging.info(f'Tree contains {len(tree_leaves)} leaves (i.e., genera).')\n",
    "logging.info(f\"{tree_leaves.str.endswith('sp.').sum()} species names are not defined (e.g., 'Lessertia_sp.')\")\n",
    "\n",
    "\n",
    "#create SPARQL query for given genus\n",
    "def generate_query(genus):\n",
    "    return f\"\"\"\n",
    "    PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
    "    PREFIX p: <http://www.wikidata.org/prop/>\n",
    "    PREFIX ps: <http://www.wikidata.org/prop/statement/>\n",
    "    PREFIX pr: <http://www.wikidata.org/prop/reference/>\n",
    "    PREFIX prov: <http://www.w3.org/ns/prov#>\n",
    "\n",
    "    SELECT DISTINCT ?genus ?genus_name ?taxon ?taxon_name ?structure_inchikey ?structure_smiles \n",
    "    (GROUP_CONCAT(DISTINCT ?reference; separator=\", \") AS ?references) \n",
    "    (GROUP_CONCAT(DISTINCT ?reference_doi; separator=\", \") AS ?reference_dois) WHERE {{\n",
    "        ?genus wdt:P225 \"{genus}\".\n",
    "        ?genus wdt:P225 ?genus_name.                 \n",
    "\n",
    "        ?taxon wdt:P171* ?genus.                     \n",
    "        ?structure wdt:P235 ?structure_inchikey;      \n",
    "                   wdt:P233 ?structure_smiles;        \n",
    "                   p:P703 [                           \n",
    "                       ps:P703 ?taxon;                \n",
    "                       prov:wasDerivedFrom/pr:P248 ?reference  \n",
    "                   ].\n",
    "        ?taxon wdt:P225 ?taxon_name.                  \n",
    "        OPTIONAL {{ ?reference wdt:P356 ?reference_doi. }}\n",
    "    }}\n",
    "    GROUP BY ?genus ?genus_name ?taxon ?taxon_name ?structure_inchikey ?structure_smiles\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "#run (generated) SPARQL query for a given genus\n",
    "def run_query(genus, max_attempts=5):\n",
    "    \n",
    "    #generate query\n",
    "    query = generate_query(genus)\n",
    "    url = \"https://query.wikidata.org/sparql\"\n",
    "    headers = {'Accept': 'application/sparql-results+json',\n",
    "               'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "    \n",
    "    #try to run query\n",
    "    for attempt in range(1, max_attempts + 1):\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, params={'query': query})\n",
    "            response.raise_for_status() #raise exception for HTTP response 4xx or 5xx (error)\n",
    "            query_out = response.json()\n",
    "            out_df = pd.json_normalize(query_out['results']['bindings']) #convert to df\n",
    "            out_df = out_df[[col for col in out_df.columns if col.endswith('.value')]]\n",
    "            out_df.columns = [col.replace('.value', '') for col in out_df.columns]\n",
    "            \n",
    "            #successful query: log and return output\n",
    "            logging.info(f\"Query for '{genus}' genus completed in {attempt} attempts.\")\n",
    "            return out_df\n",
    "        \n",
    "        except HTTPError as http_err:\n",
    "            if response.status_code == 429:  #HTTP Error 429: Too Many Requests\n",
    "                wait_time = 2 ** attempt  #exponential backoff\n",
    "                logging.warning(f\"Wikidata requests limit reached when querying '{genus}' genus. Retrying in {wait_time} seconds...\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                logging.error(f\"HTTP Error when querying genus '{genus}' on attempt NÂ° {attempt}: {http_err}\")\n",
    "                break #break loop if error differen from HTTPError429 occurs\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Unexpected error for genus '{genus}' on attempt {attempt}: {e}\")\n",
    "            break #break loop if any other error occurs\n",
    "\n",
    "    #query unsuccessful (nothing returned in the for loop): log and return None\n",
    "    logging.error(f\"Query for '{genus}' genus failed after {max_attempts} attempts.\")\n",
    "    return None  # Return None to indicate a failed request\n",
    "\n",
    "\n",
    "# Process genera in parallel\n",
    "def process_genera_parallel(genera_list, threads=20, max_attempts=5):\n",
    "    all_results = []\n",
    "    failed_tasks = {}\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=threads) as executor:\n",
    "        # Submit tasks\n",
    "        tasks_list = {executor.submit(run_query, genus, max_attempts): genus for genus in genera_list}\n",
    "\n",
    "        for completed_task in as_completed(tasks_list):\n",
    "            genus_name = tasks_list[completed_task]\n",
    "            try:\n",
    "                output = completed_task.result()\n",
    "                if output is not None:  # Only add successful results\n",
    "                    all_results.append(output)\n",
    "                else:\n",
    "                    failed_tasks[genus_name] = \"No output returned\"\n",
    "\n",
    "            except Exception as e:\n",
    "                failed_tasks[genus_name] = str(e)\n",
    "                \n",
    "\n",
    "    #Log query summary\n",
    "    logging.info(f\"Processing completed: {len(genera_list)} queries run.\")\n",
    "    logging.info(f\"{len(genera_list) - len(failed_tasks)} queries were successfully.\")\n",
    "    if failed_tasks:\n",
    "        logging.warning(f\"The following {len(failed_tasks)} queries failed:\")\n",
    "        for genus, reason in failed_tasks.items():\n",
    "            logging.error(f\"'{genus}' genus: {reason}\")\n",
    "\n",
    "    #concatenate all results into a single dataframe\n",
    "    if all_results:\n",
    "        return pd.concat(all_results, ignore_index=True), failed_tasks\n",
    "    else:\n",
    "        logging.warning(\"No results returned.\")\n",
    "        return pd.DataFrame(), failed_tasks  # Return empty dataframe if no results\n",
    "    \n",
    "#Run queries (parallelized)\n",
    "results_df, failed_tasks = process_genera_parallel(genera_list, threads=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tropicana",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
